{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HimashiRathnayake/Sinhala-Text-Simplification-Dataset-and-Evaluation/blob/main/mT5/fine_tune_mt5_en_simp_si_simp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **mT5 fine tuning for English and Sinhala text simplification**"
      ],
      "metadata": {
        "id": "-4_jh2v2b6rH"
      },
      "id": "-4_jh2v2b6rH"
    },
    {
      "cell_type": "markdown",
      "id": "ef3721dd",
      "metadata": {
        "id": "ef3721dd",
        "papermill": {
          "duration": 0.009568,
          "end_time": "2022-01-09T05:03:53.815288",
          "exception": false,
          "start_time": "2022-01-09T05:03:53.805720",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Import Git repo and install libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fcdddc1",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T05:03:53.838165Z",
          "iopub.status.busy": "2022-01-09T05:03:53.836656Z",
          "iopub.status.idle": "2022-01-09T05:04:51.955217Z",
          "shell.execute_reply": "2022-01-09T05:04:51.954588Z",
          "shell.execute_reply.started": "2021-12-18T15:24:59.015414Z"
        },
        "papermill": {
          "duration": 58.131071,
          "end_time": "2022-01-09T05:04:51.955377",
          "exception": false,
          "start_time": "2022-01-09T05:03:53.824306",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fcdddc1",
        "outputId": "c37b89d8-d365-48ae-ab1b-62453122a3a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mt5-simplification'...\n",
            "remote: Enumerating objects: 282, done.\u001b[K\n",
            "remote: Total 282 (delta 0), reused 0 (delta 0), pack-reused 282\u001b[K\n",
            "Receiving objects: 100% (282/282), 729.81 KiB | 15.87 MiB/s, done.\n",
            "Resolving deltas: 100% (168/168), done.\n",
            "Cloning into 'transformers'...\n",
            "remote: Enumerating objects: 102756, done.\u001b[K\n",
            "remote: Counting objects: 100% (641/641), done.\u001b[K\n",
            "remote: Compressing objects: 100% (249/249), done.\u001b[K\n",
            "remote: Total 102756 (delta 389), reused 463 (delta 319), pack-reused 102115\u001b[K\n",
            "Receiving objects: 100% (102756/102756), 96.08 MiB | 20.88 MiB/s, done.\n",
            "Resolving deltas: 100% (75801/75801), done.\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 7.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 59.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 298 kB 9.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 54 kB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 123 kB 71.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 65 kB 3.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 141 kB 61.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 59.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 17.1 MB 322 kB/s \n",
            "\u001b[K     |████████████████████████████████| 138 kB 55.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.3 MB 41.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.4 MB 44.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 357 kB 59.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/rumeshmadhusanka/mt5-simplification.git\n",
        "!git clone https://github.com/huggingface/transformers.git\n",
        "! cd transformers && \\\n",
        "pip install --editable ./ -q\n",
        "!pip install gdown datasets==1.16.1 sacrebleu==1.4.12 sentencepiece==0.1.96 protobuf accelerate py7zr -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12f649e3",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T05:04:52.048885Z",
          "iopub.status.busy": "2022-01-09T05:04:52.048090Z",
          "iopub.status.idle": "2022-01-09T05:05:18.875384Z",
          "shell.execute_reply": "2022-01-09T05:05:18.874870Z",
          "shell.execute_reply.started": "2021-12-18T15:26:10.3766Z"
        },
        "papermill": {
          "duration": 26.876658,
          "end_time": "2022-01-09T05:05:18.875521",
          "exception": false,
          "start_time": "2022-01-09T05:04:51.998863",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "12f649e3",
        "outputId": "deca7c90-37e5-45a0-ceea-1c410a74f76d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.5 MB 8.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 76 kB 4.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 20.1 MB 1.4 MB/s \n",
            "\u001b[?25h  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.18.5 which is incompatible.\n",
            "tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n",
            "jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "jax 0.3.14 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 61.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 68.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        }
      ],
      "source": [
        "!pip install -r mt5-simplification/requirements.txt -q\n",
        "!pip install gdown wandb -q\n",
        "!wandb login 05f8f12ea26899b0e6d9398182dc1f217c345e08"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5045f895",
      "metadata": {
        "papermill": {
          "duration": 0.045038,
          "end_time": "2022-01-09T05:05:18.964352",
          "exception": false,
          "start_time": "2022-01-09T05:05:18.919314",
          "status": "completed"
        },
        "tags": [],
        "id": "5045f895"
      },
      "source": [
        "### En simplification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd042f42",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T05:05:19.058444Z",
          "iopub.status.busy": "2022-01-09T05:05:19.057672Z",
          "iopub.status.idle": "2022-01-09T05:05:36.830511Z",
          "shell.execute_reply": "2022-01-09T05:05:36.829882Z"
        },
        "papermill": {
          "duration": 17.822235,
          "end_time": "2022-01-09T05:05:36.830636",
          "exception": false,
          "start_time": "2022-01-09T05:05:19.008401",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd042f42",
        "outputId": "42d23240-4a3f-4111-abe5-74db3ddc2ca8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
            "  category=FutureWarning,\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1kSgHhnxYC8QNy9e2HMZO6n0g4yhkhw-a\n",
            "To: /content/newsela-en-json.tar.xz\n",
            "100% 2.80M/2.80M [00:00<00:00, 196MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown --id 1kSgHhnxYC8QNy9e2HMZO6n0g4yhkhw-a\n",
        "!tar xf newsela-en-json.tar.xz"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c9553ab",
      "metadata": {
        "id": "0c9553ab",
        "papermill": {
          "duration": 0.045018,
          "end_time": "2022-01-09T05:05:36.920528",
          "exception": false,
          "start_time": "2022-01-09T05:05:36.875510",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "### Si simplification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f650aae",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T05:05:37.016718Z",
          "iopub.status.busy": "2022-01-09T05:05:37.015880Z",
          "iopub.status.idle": "2022-01-09T05:05:37.018550Z",
          "shell.execute_reply": "2022-01-09T05:05:37.018094Z",
          "shell.execute_reply.started": "2021-12-18T15:35:39.756464Z"
        },
        "papermill": {
          "duration": 0.052125,
          "end_time": "2022-01-09T05:05:37.018662",
          "exception": false,
          "start_time": "2022-01-09T05:05:36.966537",
          "status": "completed"
        },
        "tags": [],
        "id": "3f650aae"
      },
      "outputs": [],
      "source": [
        "# !gdown --id 13cRqzFJMLbhIj-0T9ikIzBr_xB7vAuOP\n",
        "# !tar xf newsela-si-json.tar.xz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35ce3127",
      "metadata": {
        "_kg_hide-input": true,
        "execution": {
          "iopub.execute_input": "2022-01-09T05:05:37.118297Z",
          "iopub.status.busy": "2022-01-09T05:05:37.117443Z",
          "iopub.status.idle": "2022-01-09T11:24:25.093447Z",
          "shell.execute_reply": "2022-01-09T11:24:25.092876Z",
          "shell.execute_reply.started": "2021-12-18T15:38:08.196637Z"
        },
        "id": "35ce3127",
        "outputId": "4dff9a2f-eb2c-4f0d-dd68-4714174bd75d",
        "papermill": {
          "duration": 22728.029268,
          "end_time": "2022-01-09T11:24:25.093590",
          "exception": false,
          "start_time": "2022-01-09T05:05:37.064322",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset json/default to /root/.cache/huggingface/datasets/json/default-f35ca72253267dd0/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426...\r\n",
            "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 8050.49it/s]\r\n",
            "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1319.38it/s]\r\n",
            "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/json/default-f35ca72253267dd0/0.0.0/c2d554c3377ea79c7664b93dc65d0803b45e3279000f993c7bfd18937fd7f426. Subsequent calls will reuse this data.\r\n",
            "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 373.16it/s]\r\n",
            "[INFO|file_utils.py:2042] 2022-01-09 05:05:47,666 >> https://huggingface.co/google/mt5-base/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp13_iwd1n\r\n",
            "Downloading: 100%|██████████████████████████████| 702/702 [00:00<00:00, 558kB/s]\r\n",
            "[INFO|file_utils.py:2046] 2022-01-09 05:05:48,003 >> storing https://huggingface.co/google/mt5-base/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de\r\n",
            "[INFO|file_utils.py:2054] 2022-01-09 05:05:48,003 >> creating metadata file for /root/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de\r\n",
            "[INFO|configuration_utils.py:609] 2022-01-09 05:05:48,004 >> loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de\r\n",
            "[INFO|configuration_utils.py:645] 2022-01-09 05:05:48,006 >> Model config MT5Config {\r\n",
            "  \"_name_or_path\": \"google/mt5-base\",\r\n",
            "  \"architectures\": [\r\n",
            "    \"MT5ForConditionalGeneration\"\r\n",
            "  ],\r\n",
            "  \"d_ff\": 2048,\r\n",
            "  \"d_kv\": 64,\r\n",
            "  \"d_model\": 768,\r\n",
            "  \"decoder_start_token_id\": 0,\r\n",
            "  \"dropout_rate\": 0.1,\r\n",
            "  \"eos_token_id\": 1,\r\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
            "  \"initializer_factor\": 1.0,\r\n",
            "  \"is_encoder_decoder\": true,\r\n",
            "  \"layer_norm_epsilon\": 1e-06,\r\n",
            "  \"model_type\": \"mt5\",\r\n",
            "  \"num_decoder_layers\": 12,\r\n",
            "  \"num_heads\": 12,\r\n",
            "  \"num_layers\": 12,\r\n",
            "  \"output_past\": true,\r\n",
            "  \"pad_token_id\": 0,\r\n",
            "  \"relative_attention_num_buckets\": 32,\r\n",
            "  \"tie_word_embeddings\": false,\r\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\r\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\r\n",
            "  \"use_cache\": true,\r\n",
            "  \"vocab_size\": 250112\r\n",
            "}\r\n",
            "\r\n",
            "[INFO|file_utils.py:2042] 2022-01-09 05:05:48,700 >> https://huggingface.co/google/mt5-base/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp636x5y97\r\n",
            "Downloading: 100%|██████████████████████████████| 376/376 [00:00<00:00, 280kB/s]\r\n",
            "[INFO|file_utils.py:2046] 2022-01-09 05:05:49,038 >> storing https://huggingface.co/google/mt5-base/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/afba33be693521ccefbde6d03b93b5c517d7108ba31f6c08000ed52c2cea45c9.28bbf90ae7962b1b7211c0ce8b2006f968c82439ec9c47e0847ba63642f9435a\r\n",
            "[INFO|file_utils.py:2054] 2022-01-09 05:05:49,038 >> creating metadata file for /root/.cache/huggingface/transformers/afba33be693521ccefbde6d03b93b5c517d7108ba31f6c08000ed52c2cea45c9.28bbf90ae7962b1b7211c0ce8b2006f968c82439ec9c47e0847ba63642f9435a\r\n",
            "[INFO|configuration_utils.py:609] 2022-01-09 05:05:49,713 >> loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de\r\n",
            "[INFO|configuration_utils.py:645] 2022-01-09 05:05:49,714 >> Model config MT5Config {\r\n",
            "  \"_name_or_path\": \"google/mt5-base\",\r\n",
            "  \"architectures\": [\r\n",
            "    \"MT5ForConditionalGeneration\"\r\n",
            "  ],\r\n",
            "  \"d_ff\": 2048,\r\n",
            "  \"d_kv\": 64,\r\n",
            "  \"d_model\": 768,\r\n",
            "  \"decoder_start_token_id\": 0,\r\n",
            "  \"dropout_rate\": 0.1,\r\n",
            "  \"eos_token_id\": 1,\r\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
            "  \"initializer_factor\": 1.0,\r\n",
            "  \"is_encoder_decoder\": true,\r\n",
            "  \"layer_norm_epsilon\": 1e-06,\r\n",
            "  \"model_type\": \"mt5\",\r\n",
            "  \"num_decoder_layers\": 12,\r\n",
            "  \"num_heads\": 12,\r\n",
            "  \"num_layers\": 12,\r\n",
            "  \"output_past\": true,\r\n",
            "  \"pad_token_id\": 0,\r\n",
            "  \"relative_attention_num_buckets\": 32,\r\n",
            "  \"tie_word_embeddings\": false,\r\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\r\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\r\n",
            "  \"use_cache\": true,\r\n",
            "  \"vocab_size\": 250112\r\n",
            "}\r\n",
            "\r\n",
            "[INFO|file_utils.py:2042] 2022-01-09 05:05:50,397 >> https://huggingface.co/google/mt5-base/resolve/main/spiece.model not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp1m9bmjmt\r\n",
            "Downloading: 100%|█████████████████████████| 4.11M/4.11M [00:00<00:00, 8.36MB/s]\r\n",
            "[INFO|file_utils.py:2046] 2022-01-09 05:05:51,355 >> storing https://huggingface.co/google/mt5-base/resolve/main/spiece.model in cache at /root/.cache/huggingface/transformers/4764ec347af4d2d6286acbe1d9d630ac0afd8554a4c4a64170e0b663fd2e2412.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\r\n",
            "[INFO|file_utils.py:2054] 2022-01-09 05:05:51,355 >> creating metadata file for /root/.cache/huggingface/transformers/4764ec347af4d2d6286acbe1d9d630ac0afd8554a4c4a64170e0b663fd2e2412.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\r\n",
            "[DEBUG|tokenization_utils_base.py:1709] 2022-01-09 05:05:51,692 >> 404 Client Error: Not Found for url: https://huggingface.co/google/mt5-base/resolve/main/tokenizer.json\r\n",
            "[DEBUG|tokenization_utils_base.py:1709] 2022-01-09 05:05:52,029 >> 404 Client Error: Not Found for url: https://huggingface.co/google/mt5-base/resolve/main/added_tokens.json\r\n",
            "[INFO|file_utils.py:2042] 2022-01-09 05:05:52,367 >> https://huggingface.co/google/mt5-base/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpv5e6aims\r\n",
            "Downloading: 100%|███████████████████████████| 65.0/65.0 [00:00<00:00, 45.6kB/s]\r\n",
            "[INFO|file_utils.py:2046] 2022-01-09 05:05:52,704 >> storing https://huggingface.co/google/mt5-base/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/0d7d5b3fc19bf58d4b274990c8bcf5e307726bc18d95f40a1436dfb6a0892f85.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82\r\n",
            "[INFO|file_utils.py:2054] 2022-01-09 05:05:52,704 >> creating metadata file for /root/.cache/huggingface/transformers/0d7d5b3fc19bf58d4b274990c8bcf5e307726bc18d95f40a1436dfb6a0892f85.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82\r\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-09 05:05:53,039 >> loading file https://huggingface.co/google/mt5-base/resolve/main/spiece.model from cache at /root/.cache/huggingface/transformers/4764ec347af4d2d6286acbe1d9d630ac0afd8554a4c4a64170e0b663fd2e2412.84ea7af2df68dc8db434d3160aab65cce8ac63ce5b6f7743f8c9a4a14b4f77e2\r\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-09 05:05:53,040 >> loading file https://huggingface.co/google/mt5-base/resolve/main/tokenizer.json from cache at None\r\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-09 05:05:53,040 >> loading file https://huggingface.co/google/mt5-base/resolve/main/added_tokens.json from cache at None\r\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-09 05:05:53,040 >> loading file https://huggingface.co/google/mt5-base/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/0d7d5b3fc19bf58d4b274990c8bcf5e307726bc18d95f40a1436dfb6a0892f85.294ebaa4cd17bb284635004c92d2c4d522ec488c828dcce0c2471b6f28e3fe82\r\n",
            "[INFO|tokenization_utils_base.py:1740] 2022-01-09 05:05:53,040 >> loading file https://huggingface.co/google/mt5-base/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/afba33be693521ccefbde6d03b93b5c517d7108ba31f6c08000ed52c2cea45c9.28bbf90ae7962b1b7211c0ce8b2006f968c82439ec9c47e0847ba63642f9435a\r\n",
            "[INFO|configuration_utils.py:609] 2022-01-09 05:05:53,716 >> loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de\r\n",
            "[INFO|configuration_utils.py:645] 2022-01-09 05:05:53,717 >> Model config MT5Config {\r\n",
            "  \"_name_or_path\": \"google/mt5-base\",\r\n",
            "  \"architectures\": [\r\n",
            "    \"MT5ForConditionalGeneration\"\r\n",
            "  ],\r\n",
            "  \"d_ff\": 2048,\r\n",
            "  \"d_kv\": 64,\r\n",
            "  \"d_model\": 768,\r\n",
            "  \"decoder_start_token_id\": 0,\r\n",
            "  \"dropout_rate\": 0.1,\r\n",
            "  \"eos_token_id\": 1,\r\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
            "  \"initializer_factor\": 1.0,\r\n",
            "  \"is_encoder_decoder\": true,\r\n",
            "  \"layer_norm_epsilon\": 1e-06,\r\n",
            "  \"model_type\": \"mt5\",\r\n",
            "  \"num_decoder_layers\": 12,\r\n",
            "  \"num_heads\": 12,\r\n",
            "  \"num_layers\": 12,\r\n",
            "  \"output_past\": true,\r\n",
            "  \"pad_token_id\": 0,\r\n",
            "  \"relative_attention_num_buckets\": 32,\r\n",
            "  \"tie_word_embeddings\": false,\r\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\r\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\r\n",
            "  \"use_cache\": true,\r\n",
            "  \"vocab_size\": 250112\r\n",
            "}\r\n",
            "\r\n",
            "[INFO|configuration_utils.py:609] 2022-01-09 05:05:55,056 >> loading configuration file https://huggingface.co/google/mt5-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/5ebfd830555547194403d6803baa127970de59b443c04b7a1a60b16a97ed3958.b589da7dac64196f9764abaf2c4c7e507cec8b14b96da3ef270d924f155062de\r\n",
            "[INFO|configuration_utils.py:645] 2022-01-09 05:05:55,057 >> Model config MT5Config {\r\n",
            "  \"_name_or_path\": \"google/mt5-base\",\r\n",
            "  \"architectures\": [\r\n",
            "    \"MT5ForConditionalGeneration\"\r\n",
            "  ],\r\n",
            "  \"d_ff\": 2048,\r\n",
            "  \"d_kv\": 64,\r\n",
            "  \"d_model\": 768,\r\n",
            "  \"decoder_start_token_id\": 0,\r\n",
            "  \"dropout_rate\": 0.1,\r\n",
            "  \"eos_token_id\": 1,\r\n",
            "  \"feed_forward_proj\": \"gated-gelu\",\r\n",
            "  \"initializer_factor\": 1.0,\r\n",
            "  \"is_encoder_decoder\": true,\r\n",
            "  \"layer_norm_epsilon\": 1e-06,\r\n",
            "  \"model_type\": \"mt5\",\r\n",
            "  \"num_decoder_layers\": 12,\r\n",
            "  \"num_heads\": 12,\r\n",
            "  \"num_layers\": 12,\r\n",
            "  \"output_past\": true,\r\n",
            "  \"pad_token_id\": 0,\r\n",
            "  \"relative_attention_num_buckets\": 32,\r\n",
            "  \"tie_word_embeddings\": false,\r\n",
            "  \"tokenizer_class\": \"T5Tokenizer\",\r\n",
            "  \"transformers_version\": \"4.16.0.dev0\",\r\n",
            "  \"use_cache\": true,\r\n",
            "  \"vocab_size\": 250112\r\n",
            "}\r\n",
            "\r\n",
            "[INFO|file_utils.py:2042] 2022-01-09 05:05:56,518 >> https://huggingface.co/google/mt5-base/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpzfzov_ws\r\n",
            "Downloading: 100%|█████████████████████████| 2.17G/2.17G [01:41<00:00, 22.9MB/s]\r\n",
            "[INFO|file_utils.py:2046] 2022-01-09 05:07:38,732 >> storing https://huggingface.co/google/mt5-base/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/3b7e8056d4ed71d8d7ac2dea78627c4be77ed136399c05b563d4116abfcd9418.1afec9001b62cd5a347e7fd4b664e503ca2377606e11b9ddb8ec1d7b79bc3952\r\n",
            "[INFO|file_utils.py:2054] 2022-01-09 05:07:38,732 >> creating metadata file for /root/.cache/huggingface/transformers/3b7e8056d4ed71d8d7ac2dea78627c4be77ed136399c05b563d4116abfcd9418.1afec9001b62cd5a347e7fd4b664e503ca2377606e11b9ddb8ec1d7b79bc3952\r\n",
            "[INFO|modeling_utils.py:1353] 2022-01-09 05:07:38,733 >> loading weights file https://huggingface.co/google/mt5-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/3b7e8056d4ed71d8d7ac2dea78627c4be77ed136399c05b563d4116abfcd9418.1afec9001b62cd5a347e7fd4b664e503ca2377606e11b9ddb8ec1d7b79bc3952\r\n",
            "[INFO|modeling_utils.py:1620] 2022-01-09 05:07:47,023 >> All model checkpoint weights were used when initializing MT5ForConditionalGeneration.\r\n",
            "\r\n",
            "[INFO|modeling_utils.py:1629] 2022-01-09 05:07:47,023 >> All the weights of MT5ForConditionalGeneration were initialized from the model checkpoint at google/mt5-base.\r\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use MT5ForConditionalGeneration for predictions without further training.\r\n",
            "Running tokenizer on train dataset:   0%|               | 0/140 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   1%|       | 1/140 [00:00<00:32,  4.24ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   1%|       | 2/140 [00:00<00:26,  5.23ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   2%|▏      | 3/140 [00:00<00:24,  5.59ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   3%|▏      | 4/140 [00:00<00:23,  5.86ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   4%|▎      | 5/140 [00:00<00:22,  6.01ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   4%|▎      | 6/140 [00:01<00:21,  6.14ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   5%|▎      | 7/140 [00:01<00:21,  6.21ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   6%|▍      | 8/140 [00:01<00:20,  6.29ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   6%|▍      | 9/140 [00:01<00:20,  6.29ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   7%|▍     | 10/140 [00:01<00:27,  4.74ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   8%|▍     | 11/140 [00:02<00:25,  4.97ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   9%|▌     | 12/140 [00:02<00:24,  5.22ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:   9%|▌     | 13/140 [00:02<00:23,  5.31ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  10%|▌     | 14/140 [00:02<00:22,  5.50ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  11%|▋     | 15/140 [00:02<00:22,  5.57ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  11%|▋     | 16/140 [00:02<00:22,  5.56ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  12%|▋     | 17/140 [00:03<00:21,  5.65ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  13%|▊     | 18/140 [00:03<00:21,  5.65ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  14%|▊     | 19/140 [00:03<00:21,  5.70ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  14%|▊     | 20/140 [00:03<00:20,  5.72ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  15%|▉     | 21/140 [00:03<00:20,  5.68ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  16%|▉     | 22/140 [00:04<00:24,  4.77ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  16%|▉     | 23/140 [00:04<00:23,  5.01ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  17%|█     | 24/140 [00:04<00:22,  5.16ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  18%|█     | 25/140 [00:04<00:21,  5.34ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  19%|█     | 26/140 [00:04<00:20,  5.46ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  19%|█▏    | 27/140 [00:04<00:20,  5.54ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  20%|█▏    | 28/140 [00:05<00:20,  5.58ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  21%|█▏    | 29/140 [00:05<00:19,  5.57ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  21%|█▎    | 30/140 [00:05<00:20,  5.49ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  22%|█▎    | 31/140 [00:05<00:19,  5.60ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  23%|█▎    | 32/140 [00:06<00:26,  4.09ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  24%|█▍    | 33/140 [00:06<00:27,  3.92ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  24%|█▍    | 34/140 [00:06<00:26,  3.95ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  25%|█▌    | 35/140 [00:06<00:24,  4.31ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  26%|█▌    | 36/140 [00:06<00:22,  4.54ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  26%|█▌    | 37/140 [00:07<00:21,  4.83ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  27%|█▋    | 38/140 [00:07<00:20,  5.07ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  28%|█▋    | 39/140 [00:07<00:18,  5.32ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  29%|█▋    | 40/140 [00:07<00:18,  5.49ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  29%|█▊    | 41/140 [00:07<00:18,  5.43ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  30%|█▊    | 42/140 [00:07<00:17,  5.47ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  31%|█▊    | 43/140 [00:08<00:17,  5.56ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  31%|█▉    | 44/140 [00:08<00:20,  4.78ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  32%|█▉    | 45/140 [00:08<00:18,  5.09ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  33%|█▉    | 46/140 [00:08<00:17,  5.28ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  34%|██    | 47/140 [00:08<00:17,  5.47ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  34%|██    | 48/140 [00:09<00:16,  5.55ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  35%|██    | 49/140 [00:09<00:16,  5.54ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  36%|██▏   | 50/140 [00:09<00:16,  5.54ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  36%|██▏   | 51/140 [00:09<00:15,  5.59ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  37%|██▏   | 52/140 [00:09<00:15,  5.53ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  38%|██▎   | 53/140 [00:10<00:15,  5.51ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  39%|██▎   | 54/140 [00:10<00:18,  4.68ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  39%|██▎   | 55/140 [00:10<00:16,  5.01ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  40%|██▍   | 56/140 [00:10<00:16,  5.21ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  41%|██▍   | 57/140 [00:10<00:15,  5.33ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  41%|██▍   | 58/140 [00:11<00:15,  5.43ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  42%|██▌   | 59/140 [00:11<00:14,  5.48ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  43%|██▌   | 60/140 [00:11<00:14,  5.60ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  44%|██▌   | 61/140 [00:11<00:14,  5.62ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  44%|██▋   | 62/140 [00:11<00:14,  5.49ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  45%|██▋   | 63/140 [00:11<00:13,  5.57ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  46%|██▋   | 64/140 [00:12<00:13,  5.56ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  46%|██▊   | 65/140 [00:12<00:18,  4.11ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  47%|██▊   | 66/140 [00:12<00:19,  3.80ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  48%|██▊   | 67/140 [00:13<00:20,  3.51ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  49%|██▉   | 68/140 [00:13<00:20,  3.48ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  49%|██▉   | 69/140 [00:13<00:17,  3.99ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  50%|███   | 70/140 [00:13<00:15,  4.43ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  51%|███   | 71/140 [00:13<00:14,  4.77ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  51%|███   | 72/140 [00:14<00:13,  5.00ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  52%|███▏  | 73/140 [00:14<00:12,  5.18ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  53%|███▏  | 74/140 [00:14<00:12,  5.37ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  54%|███▏  | 75/140 [00:14<00:11,  5.48ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  54%|███▎  | 76/140 [00:14<00:11,  5.55ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  55%|███▎  | 77/140 [00:15<00:13,  4.68ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  56%|███▎  | 78/140 [00:15<00:12,  4.99ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  56%|███▍  | 79/140 [00:15<00:11,  5.24ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  57%|███▍  | 80/140 [00:15<00:11,  5.37ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  58%|███▍  | 81/140 [00:15<00:10,  5.46ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  59%|███▌  | 82/140 [00:15<00:10,  5.58ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  59%|███▌  | 83/140 [00:16<00:10,  5.59ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  60%|███▌  | 84/140 [00:16<00:10,  5.56ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  61%|███▋  | 85/140 [00:16<00:09,  5.59ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  61%|███▋  | 86/140 [00:16<00:09,  5.64ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  62%|███▋  | 87/140 [00:17<00:14,  3.64ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  63%|███▊  | 88/140 [00:17<00:13,  3.76ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  64%|███▊  | 89/140 [00:17<00:12,  4.03ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  64%|███▊  | 90/140 [00:17<00:12,  4.04ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  65%|███▉  | 91/140 [00:18<00:13,  3.74ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  66%|███▉  | 92/140 [00:18<00:11,  4.01ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  66%|███▉  | 93/140 [00:18<00:10,  4.39ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  67%|████  | 94/140 [00:18<00:09,  4.72ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  68%|████  | 95/140 [00:18<00:09,  4.99ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  69%|████  | 96/140 [00:19<00:08,  5.25ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  69%|████▏ | 97/140 [00:19<00:07,  5.49ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  70%|████▏ | 98/140 [00:19<00:09,  4.62ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  71%|████▏ | 99/140 [00:19<00:08,  4.89ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  71%|███▌ | 100/140 [00:19<00:07,  5.07ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  72%|███▌ | 101/140 [00:20<00:07,  5.24ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  73%|███▋ | 102/140 [00:20<00:07,  5.36ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  74%|███▋ | 103/140 [00:20<00:06,  5.49ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  74%|███▋ | 104/140 [00:20<00:06,  5.63ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  75%|███▊ | 105/140 [00:20<00:06,  5.69ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  76%|███▊ | 106/140 [00:20<00:05,  5.74ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  76%|███▊ | 107/140 [00:21<00:05,  5.88ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  77%|███▊ | 108/140 [00:21<00:05,  5.88ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  78%|███▉ | 109/140 [00:21<00:05,  5.85ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  79%|███▉ | 110/140 [00:21<00:06,  4.74ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  79%|███▉ | 111/140 [00:21<00:05,  4.94ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  80%|████ | 112/140 [00:22<00:05,  5.14ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  81%|████ | 113/140 [00:22<00:05,  5.25ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  81%|████ | 114/140 [00:22<00:04,  5.38ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  82%|████ | 115/140 [00:22<00:04,  5.48ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  83%|████▏| 116/140 [00:22<00:04,  5.58ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  84%|████▏| 117/140 [00:22<00:04,  5.57ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  84%|████▏| 118/140 [00:23<00:03,  5.64ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  85%|████▎| 119/140 [00:23<00:03,  5.76ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  86%|████▎| 120/140 [00:23<00:04,  4.81ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  86%|████▎| 121/140 [00:23<00:03,  5.08ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  87%|████▎| 122/140 [00:23<00:03,  5.26ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  88%|████▍| 123/140 [00:24<00:03,  5.39ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  89%|████▍| 124/140 [00:24<00:02,  5.45ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  89%|████▍| 125/140 [00:24<00:02,  5.52ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  90%|████▌| 126/140 [00:24<00:02,  5.63ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  91%|████▌| 127/140 [00:24<00:02,  5.79ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  91%|████▌| 128/140 [00:24<00:02,  5.89ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  92%|████▌| 129/140 [00:25<00:01,  5.94ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  93%|████▋| 130/140 [00:25<00:01,  5.95ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  94%|████▋| 131/140 [00:25<00:01,  4.95ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  94%|████▋| 132/140 [00:25<00:01,  5.19ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  95%|████▊| 133/140 [00:25<00:01,  5.36ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  96%|████▊| 134/140 [00:26<00:01,  5.56ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  96%|████▊| 135/140 [00:26<00:00,  5.75ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  97%|████▊| 136/140 [00:26<00:00,  5.62ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  98%|████▉| 137/140 [00:26<00:00,  5.51ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  99%|████▉| 138/140 [00:26<00:00,  5.51ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset:  99%|████▉| 139/140 [00:26<00:00,  5.52ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on train dataset: 100%|█████| 140/140 [00:27<00:00,  5.17ba/s]\r\n",
            "Running tokenizer on validation dataset:   0%|            | 0/2 [00:00<?, ?ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on validation dataset:  50%|██  | 1/2 [00:00<00:00,  5.84ba/s]Ignored unknown kwarg option direction\r\n",
            "Ignored unknown kwarg option direction\r\n",
            "Running tokenizer on validation dataset: 100%|████| 2/2 [00:00<00:00,  5.91ba/s]\r\n",
            "Downloading: 5.67kB [00:00, 3.85MB/s]                                           \r\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
            "To disable this warning, you can either:\r\n",
            "\t- Avoid using `tokenizers` before the fork if possible\r\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
            "To disable this warning, you can either:\r\n",
            "\t- Avoid using `tokenizers` before the fork if possible\r\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
            "To disable this warning, you can either:\r\n",
            "\t- Avoid using `tokenizers` before the fork if possible\r\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
            "[INFO|trainer.py:1208] 2022-01-09 05:08:24,973 >> ***** Running training *****\r\n",
            "[INFO|trainer.py:1209] 2022-01-09 05:08:24,973 >>   Num examples = 139582\r\n",
            "[INFO|trainer.py:1210] 2022-01-09 05:08:24,973 >>   Num Epochs = 2\r\n",
            "[INFO|trainer.py:1211] 2022-01-09 05:08:24,973 >>   Instantaneous batch size per device = 4\r\n",
            "[INFO|trainer.py:1212] 2022-01-09 05:08:24,973 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\r\n",
            "[INFO|trainer.py:1213] 2022-01-09 05:08:24,973 >>   Gradient Accumulation steps = 1\r\n",
            "[INFO|trainer.py:1214] 2022-01-09 05:08:24,973 >>   Total optimization steps = 69792\r\n",
            "[INFO|integrations.py:503] 2022-01-09 05:08:24,976 >> Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrumesh\u001b[0m (use `wandb login --relogin` to force relogin)\r\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
            "To disable this warning, you can either:\r\n",
            "\t- Avoid using `tokenizers` before the fork if possible\r\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\r\n",
            "To disable this warning, you can either:\r\n",
            "\t- Avoid using `tokenizers` before the fork if possible\r\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.9 is available!  To upgrade, please run:\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.7\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msimplification-en\u001b[0m\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/rumesh/huggingface\u001b[0m\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/rumesh/huggingface/runs/1l2wa5f7\u001b[0m\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in /kaggle/working/wandb/run-20220109_050825-1l2wa5f7\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run `wandb offline` to turn off syncing.\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 345... (success).\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/bleu ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/gen_len ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate ▄▇███▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss █▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second ▁\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/bleu 32.9964\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/gen_len 30.0475\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        eval/loss 1.29473\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     eval/runtime 362.4159\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/samples_per_second 5.519\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/steps_per_second 1.38\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                      train/epoch 2.0\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train/global_step 69792\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/learning_rate 0.0\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                       train/loss 1.1441\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/total_flos 3.988363698514944e+16\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 train/train_loss 1.43463\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              train/train_runtime 22164.3507\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_samples_per_second 12.595\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     train/train_steps_per_second 3.149\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33msimplification-en\u001b[0m: \u001b[34mhttps://wandb.ai/rumesh/huggingface/runs/1l2wa5f7\u001b[0m\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: ./wandb/run-20220109_050825-1l2wa5f7/logs/debug.log\r\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \r\n"
          ]
        }
      ],
      "source": [
        "#for simplification\n",
        "model=\"google/mt5-base\"\n",
        "output_dir=\"simplification-en\"\n",
        "!python mt5-simplification/finetune.py \\\n",
        "    --model_name_or_path $model \\\n",
        "    --do_train \\\n",
        "    --do_eval \\\n",
        "    --adafactor \\\n",
        "    --source_lang come \\\n",
        "    --target_lang sime \\\n",
        "    --source_prefix \"come-sime: \" \\\n",
        "    --train_file newsela-en-train.json \\\n",
        "    --validation_file newsela-en-valid.json \\\n",
        "    --output_dir mt5-simplification \\\n",
        "    --per_device_train_batch_size=4 \\\n",
        "    --per_device_eval_batch_size=4 \\\n",
        "    --save_total_limit=1 \\\n",
        "    --adam_epsilon=1e-6 \\\n",
        "    --learning_rate=3e-5 \\\n",
        "    --save_strategy=epoch \\\n",
        "    --report_to=\"wandb\" \\\n",
        "    --num_train_epochs=2 \\\n",
        "    --warmup_steps=2500 \\\n",
        "    --overwrite_output_dir \\\n",
        "    --log_level debug \\\n",
        "    --output_dir $output_dir \\\n",
        "    --predict_with_generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97812522",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T11:25:04.430293Z",
          "iopub.status.busy": "2022-01-09T11:25:04.429413Z",
          "iopub.status.idle": "2022-01-09T11:25:04.431295Z",
          "shell.execute_reply": "2022-01-09T11:25:04.431706Z",
          "shell.execute_reply.started": "2021-12-18T15:34:52.464082Z"
        },
        "papermill": {
          "duration": 19.315868,
          "end_time": "2022-01-09T11:25:04.431844",
          "exception": false,
          "start_time": "2022-01-09T11:24:45.115976",
          "status": "completed"
        },
        "tags": [],
        "id": "97812522"
      },
      "outputs": [],
      "source": [
        "# !gdown --id 1Vm26F4mAU97K7y41ADpmyjvcwhSRvVfE\n",
        "# !tar xf newsela-splitted-sineval.tar.gz\n",
        "# !rm newsela-splitted-sineval.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9f94e32",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T11:25:43.546748Z",
          "iopub.status.busy": "2022-01-09T11:25:43.545825Z",
          "iopub.status.idle": "2022-01-09T11:25:43.548282Z",
          "shell.execute_reply": "2022-01-09T11:25:43.547571Z",
          "shell.execute_reply.started": "2021-12-18T15:44:13.095382Z"
        },
        "papermill": {
          "duration": 19.664949,
          "end_time": "2022-01-09T11:25:43.548407",
          "exception": false,
          "start_time": "2022-01-09T11:25:23.883458",
          "status": "completed"
        },
        "tags": [],
        "id": "e9f94e32"
      },
      "outputs": [],
      "source": [
        "\n",
        "# task=\"com-sim\"\n",
        "# # model_path=\"google/mt5-base\"\n",
        "# !python mt5-simplification/prediction.py \\\n",
        "#     newsela-splitted-sineval/test.complex \\\n",
        "#     --model-path $output_dir \\\n",
        "#     --max_length 700 \\\n",
        "#     --output $output_dir/prediction.txt \\\n",
        "#     --count 1000 \\\n",
        "#     --verbosity h \\\n",
        "#     --task $task \\\n",
        "#     --topk 50 \\\n",
        "#     --topp 0.95 \\\n",
        "#     --num_beams 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a9e9603",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T11:26:23.157588Z",
          "iopub.status.busy": "2022-01-09T11:26:23.156640Z",
          "iopub.status.idle": "2022-01-09T11:26:23.158435Z",
          "shell.execute_reply": "2022-01-09T11:26:23.158925Z",
          "shell.execute_reply.started": "2021-12-18T15:44:34.523996Z"
        },
        "papermill": {
          "duration": 19.8176,
          "end_time": "2022-01-09T11:26:23.159063",
          "exception": false,
          "start_time": "2022-01-09T11:26:03.341463",
          "status": "completed"
        },
        "tags": [],
        "id": "9a9e9603"
      },
      "outputs": [],
      "source": [
        "# !head $output_dir/prediction.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "777d697b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2022-01-09T11:27:02.074918Z",
          "iopub.status.busy": "2022-01-09T11:27:02.074157Z",
          "iopub.status.idle": "2022-01-09T11:27:48.447244Z",
          "shell.execute_reply": "2022-01-09T11:27:48.446704Z",
          "shell.execute_reply.started": "2021-12-18T15:59:05.876549Z"
        },
        "papermill": {
          "duration": 66.279949,
          "end_time": "2022-01-09T11:27:48.447377",
          "exception": false,
          "start_time": "2022-01-09T11:26:42.167428",
          "status": "completed"
        },
        "tags": [],
        "id": "777d697b",
        "outputId": "42fb4853-ec7d-4893-96c1-2ae86226f230"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: simplification-en/ (stored 0%)\r\n",
            "  adding: simplification-en/trainer_state.json (stored 0%)\r\n",
            "  adding: simplification-en/tokenizer_config.json (stored 0%)\r\n",
            "  adding: simplification-en/eval_results.json (stored 0%)\r\n",
            "  adding: simplification-en/pytorch_model.bin (stored 0%)\r\n",
            "  adding: simplification-en/train_results.json (stored 0%)\r\n",
            "  adding: simplification-en/special_tokens_map.json (stored 0%)\r\n",
            "  adding: simplification-en/config.json (stored 0%)\r\n",
            "  adding: simplification-en/training_args.bin (stored 0%)\r\n",
            "  adding: simplification-en/tokenizer.json (stored 0%)\r\n",
            "  adding: simplification-en/spiece.model (stored 0%)\r\n",
            "  adding: simplification-en/all_results.json (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/ (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/trainer_state.json (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/tokenizer_config.json (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/pytorch_model.bin (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/rng_state.pth (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/special_tokens_map.json (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/config.json (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/training_args.bin (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/tokenizer.json (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/scheduler.pt (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/spiece.model (stored 0%)\r\n",
            "  adding: simplification-en/checkpoint-69792/optimizer.pt (stored 0%)\r\n"
          ]
        }
      ],
      "source": [
        "!zip -r -0 data.zip $output_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f33fb169",
      "metadata": {
        "papermill": {
          "duration": 19.265881,
          "end_time": "2022-01-09T11:28:28.637859",
          "exception": false,
          "start_time": "2022-01-09T11:28:09.371978",
          "status": "completed"
        },
        "tags": [],
        "id": "f33fb169"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.12"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 23104.065404,
      "end_time": "2022-01-09T11:28:49.037159",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2022-01-09T05:03:44.971755",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}